{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Problem 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.optim import Optimizer\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "# torchvision: popular datasets, model architectures, and common image transformations for computer vision.\n",
    "from torchvision import datasets\n",
    "from torchvision.transforms import transforms\n",
    "\n",
    "from random import randint\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\JAEHYEON\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\torchvision\\datasets\\mnist.py:498: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  ..\\torch\\csrc\\utils\\tensor_numpy.cpp:180.)\n",
      "  return torch.from_numpy(parsed.astype(m[2], copy=False)).view(*s)\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "Step 1: Prepare dataset\n",
    "'''\n",
    "# Use data with only 4 and 9 as labels: which is hardest to classify\n",
    "label_1, label_2 = 4, 9\n",
    "\n",
    "# MNIST training data\n",
    "train_set = datasets.MNIST(root='./mnist_data/', train=True, transform=transforms.ToTensor(), download=True)\n",
    "\n",
    "# Use data with two labels\n",
    "idx = (train_set.targets == label_1) + (train_set.targets == label_2)\n",
    "train_set.data = train_set.data[idx]\n",
    "train_set.targets = train_set.targets[idx]\n",
    "train_set.targets[train_set.targets == label_1] = -1\n",
    "train_set.targets[train_set.targets == label_2] = 1\n",
    "\n",
    "# MNIST testing data\n",
    "test_set = datasets.MNIST(root='./mnist_data/', train=False, transform=transforms.ToTensor())\n",
    "\n",
    "# Use data with two labels\n",
    "idx = (test_set.targets == label_1) + (test_set.targets == label_2)\n",
    "test_set.data = test_set.data[idx]\n",
    "test_set.targets = test_set.targets[idx]\n",
    "test_set.targets[test_set.targets == label_1] = -1\n",
    "test_set.targets[test_set.targets == label_2] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Step 2: Define the neural network class\n",
    "'''\n",
    "class LR(nn.Module) :\n",
    "    '''\n",
    "    Initialize model\n",
    "        input_dim : dimension of given input data\n",
    "    '''\n",
    "    # MNIST data is 28x28 images\n",
    "    def __init__(self, input_dim=28*28) :\n",
    "        super().__init__()\n",
    "        self.linear = nn.Linear(input_dim, 1, bias=True)\n",
    "\n",
    "    ''' forward given input x '''\n",
    "    def forward(self, x) :\n",
    "        return self.linear(x.float().view(-1, 28*28))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Step 3: specify loss function\n",
    "'''\n",
    "def logistic_loss(output, target):\n",
    "    return -torch.nn.functional.logsigmoid(target*output)\n",
    "\n",
    "def mse_loss(output, target):\n",
    "    score_4 = torch.sigmoid(-output)\n",
    "    score_9 = torch.sigmoid(output)\n",
    "    loss = (1-target) * ((1-score_4)**2 + score_9**2)\n",
    "    loss += (1+target) * (score_4**2 + (1-score_9)**2)\n",
    "    loss /= 2\n",
    "    return loss\n",
    "\n",
    "iter = 100000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Test set] Average loss: 9.9981, Accuracy: 1901/1991 (95.48%)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "Step 4: Train model with SGD and logistic loss\n",
    "'''\n",
    "model = LR()\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=1e-4)\n",
    "\n",
    "loss_function = logistic_loss\n",
    "train_loss_logistic = []\n",
    "\n",
    "for _ in range(iter) :\n",
    "    # Sample a random data for training\n",
    "    ind = randint(0, len(train_set.data)-1)\n",
    "    image, label = train_set.data[ind], train_set.targets[ind]\n",
    "\n",
    "    # Clear previously computed gradient\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    # then compute gradient with forward and backward passes\n",
    "    train_loss = loss_function(model(image), label.float())\n",
    "    train_loss_logistic.append(train_loss)\n",
    "    train_loss.backward()\n",
    "\n",
    "    # perform SGD step (parameter update)\n",
    "    optimizer.step()\n",
    "'''\n",
    "Step 5: Test model (Evaluate the accuracy)\n",
    "'''\n",
    "test_loss, correct = 0, 0\n",
    "misclassified_ind = []\n",
    "correct_ind = []\n",
    "\n",
    "# Evaluate accuracy using test data\n",
    "for ind in range(len(test_set.data)) :\n",
    "\n",
    "    image, label = test_set.data[ind], test_set.targets[ind]\n",
    "\n",
    "    # evaluate model\n",
    "    output = model(image)\n",
    "\n",
    "    # Calculate cumulative loss\n",
    "    test_loss += loss_function(output, label.float()).item()\n",
    "\n",
    "    # Make a prediction\n",
    "    if output.item() * label.item() >= 0 :\n",
    "        correct += 1\n",
    "        correct_ind += [ind]\n",
    "    else:\n",
    "        misclassified_ind += [ind]\n",
    "\n",
    "# Print out the results\n",
    "print('[Test set] Average loss: {:.4f}, Accuracy: {}/{} ({:.2f}%)\\n'.format(\n",
    "        test_loss /len(test_set.data), correct, len(test_set.data),\n",
    "        100. * correct / len(test_set.data)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Test set] Average loss: 0.1142, Accuracy: 1876/1991 (94.22%)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "Step 4: Train model with SGD and mse loss\n",
    "'''\n",
    "model = LR()\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=1e-4)\n",
    "\n",
    "loss_function = mse_loss\n",
    "train_loss_mse = []\n",
    "\n",
    "for _ in range(iter) :\n",
    "    # Sample a random data for training\n",
    "    ind = randint(0, len(train_set.data)-1)\n",
    "    image, label = train_set.data[ind], train_set.targets[ind]\n",
    "\n",
    "    # Clear previously computed gradient\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    # then compute gradient with forward and backward passes\n",
    "    train_loss = loss_function(model(image), label.float())\n",
    "    train_loss_mse.append(train_loss)\n",
    "    train_loss.backward()\n",
    "\n",
    "    # perform SGD step (parameter update)\n",
    "    optimizer.step()\n",
    "'''\n",
    "Step 5: Test model (Evaluate the accuracy)\n",
    "'''\n",
    "test_loss, correct = 0, 0\n",
    "misclassified_ind = []\n",
    "correct_ind = []\n",
    "\n",
    "# Evaluate accuracy using test data\n",
    "for ind in range(len(test_set.data)) :\n",
    "\n",
    "    image, label = test_set.data[ind], test_set.targets[ind]\n",
    "\n",
    "    # evaluate model\n",
    "    output = model(image)\n",
    "\n",
    "    # Calculate cumulative loss\n",
    "    test_loss += loss_function(output, label.float()).item()\n",
    "\n",
    "    # Make a prediction\n",
    "    if output.item() * label.item() >= 0 :\n",
    "        correct += 1\n",
    "        correct_ind += [ind]\n",
    "    else:\n",
    "        misclassified_ind += [ind]\n",
    "\n",
    "# Print out the results\n",
    "print('[Test set] Average loss: {:.4f}, Accuracy: {}/{} ({:.2f}%)\\n'.format(\n",
    "        test_loss /len(test_set.data), correct, len(test_set.data),\n",
    "        100. * correct / len(test_set.data)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "MSE Loss underperformed minimizing KL Divergence."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Problem 7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.optim import Optimizer\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets\n",
    "from torchvision.transforms import transforms\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Step 1:\n",
    "'''\n",
    "\n",
    "# MNIST dataset\n",
    "train_dataset = datasets.MNIST(root='./mnist_data/',\n",
    "                               train=True, \n",
    "                               transform=transforms.ToTensor(),\n",
    "                               download=True)\n",
    "\n",
    "test_dataset = datasets.MNIST(root='./mnist_data/',\n",
    "                              train=False, \n",
    "                              transform=transforms.ToTensor())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Step 2: LeNet5\n",
    "'''\n",
    "\n",
    "# Modern LeNet uses this layer for C3\n",
    "class C3_layer_full(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(C3_layer_full, self).__init__()\n",
    "        self.conv_layer = nn.Conv2d(6, 16, kernel_size=5)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.conv_layer(x)\n",
    "\n",
    "# Original LeNet uses this layer for C3\n",
    "class C3_layer(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(C3_layer, self).__init__()\n",
    "        self.ch_in_3 = [[0, 1, 2],\n",
    "                        [1, 2, 3],\n",
    "                        [2, 3, 4],\n",
    "                        [3, 4, 5],\n",
    "                        [0, 4, 5],\n",
    "                        [0, 1, 5]] # filter with 3 subset of input channels\n",
    "        self.ch_in_4 = [[0, 1, 2, 3],\n",
    "                        [1, 2, 3, 4],\n",
    "                        [2, 3, 4, 5],\n",
    "                        [0, 3, 4, 5],\n",
    "                        [0, 1, 4, 5],\n",
    "                        [0, 1, 2, 5],\n",
    "                        [0, 1, 3, 4],\n",
    "                        [1, 2, 4, 5],\n",
    "                        [0, 2, 3, 5]] # filter with 4 subset of input channels\n",
    "        self.ch_in_5 = [[0, 1, 2, 3, 4, 5]]\n",
    "        self.ch = self.ch_in_3 + self.ch_in_4 + self.ch_in_5\n",
    "        \n",
    "        self.convs = nn.ModuleList([nn.Conv2d(3, 1, kernel_size=5) for _ in range(6)] +\n",
    "        [nn.Conv2d(4, 1, kernel_size=5) for _ in range(9)] +\n",
    "        [nn.Conv2d(6, 1, kernel_size=5) for _ in range(1)])\n",
    "\n",
    "    def forward(self, x):\n",
    "        for i, conv in enumerate(self.convs):\n",
    "                if i == 0:\n",
    "                        output = conv(x[:, self.ch[i], :, :])\n",
    "                else:\n",
    "                        y = conv(x[:, self.ch[i], :, :])\n",
    "                        output = torch.cat((output, y), 1)\n",
    "        return output\n",
    "    \n",
    "class LeNet(nn.Module) :\n",
    "    def __init__(self) :\n",
    "        super(LeNet, self).__init__()\n",
    "        #padding=2 makes 28x28 image into 32x32\n",
    "        self.C1_layer = nn.Sequential(\n",
    "                nn.Conv2d(1, 6, kernel_size=5, padding=2),\n",
    "                nn.Tanh()\n",
    "                )\n",
    "        self.P2_layer = nn.Sequential(\n",
    "                nn.AvgPool2d(kernel_size=2, stride=2),\n",
    "                nn.Tanh()\n",
    "                )\n",
    "        self.C3_layer = nn.Sequential(\n",
    "                C3_layer_full(),\n",
    "                # C3_layer(),\n",
    "                nn.Tanh()\n",
    "                )\n",
    "        self.P4_layer = nn.Sequential(\n",
    "                nn.AvgPool2d(kernel_size=2, stride=2),\n",
    "                nn.Tanh()\n",
    "                )\n",
    "        self.C5_layer = nn.Sequential(\n",
    "                nn.Linear(5*5*16, 120),\n",
    "                nn.Tanh()\n",
    "                )\n",
    "        self.F6_layer = nn.Sequential(\n",
    "                nn.Linear(120, 84),\n",
    "                nn.Tanh()\n",
    "                )\n",
    "        self.F7_layer = nn.Linear(84, 10)\n",
    "        self.tanh = nn.Tanh()\n",
    "        \n",
    "    def forward(self, x) :\n",
    "        output = self.C1_layer(x)\n",
    "        output = self.P2_layer(output)\n",
    "        output = self.C3_layer(output)\n",
    "        output = self.P4_layer(output)\n",
    "        output = output.view(-1,5*5*16)\n",
    "        output = self.C5_layer(output)\n",
    "        output = self.F6_layer(output)\n",
    "        output = self.F7_layer(output)\n",
    "        return output\n",
    "\n",
    "class LeNet_original(nn.Module) :\n",
    "    def __init__(self) :\n",
    "        super(LeNet_original, self).__init__()\n",
    "        #padding=2 makes 28x28 image into 32x32\n",
    "        self.C1_layer = nn.Sequential(\n",
    "                nn.Conv2d(1, 6, kernel_size=5, padding=2),\n",
    "                nn.Tanh()\n",
    "                )\n",
    "        self.P2_layer = nn.Sequential(\n",
    "                nn.AvgPool2d(kernel_size=2, stride=2),\n",
    "                nn.Tanh()\n",
    "                )\n",
    "        self.C3_layer = nn.Sequential(\n",
    "                # C3_layer_full(),\n",
    "                C3_layer(),\n",
    "                nn.Tanh()\n",
    "                )\n",
    "        self.P4_layer = nn.Sequential(\n",
    "                nn.AvgPool2d(kernel_size=2, stride=2),\n",
    "                nn.Tanh()\n",
    "                )\n",
    "        self.C5_layer = nn.Sequential(\n",
    "                nn.Linear(5*5*16, 120),\n",
    "                nn.Tanh()\n",
    "                )\n",
    "        self.F6_layer = nn.Sequential(\n",
    "                nn.Linear(120, 84),\n",
    "                nn.Tanh()\n",
    "                )\n",
    "        self.F7_layer = nn.Linear(84, 10)\n",
    "        self.tanh = nn.Tanh()\n",
    "        \n",
    "    def forward(self, x) :\n",
    "        output = self.C1_layer(x)\n",
    "        output = self.P2_layer(output)\n",
    "        output = self.C3_layer(output)\n",
    "        output = self.P4_layer(output)\n",
    "        output = output.view(-1,5*5*16)\n",
    "        output = self.C5_layer(output)\n",
    "        output = self.F6_layer(output)\n",
    "        output = self.F7_layer(output)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of trainable parameters: 61706\n",
      "0th epoch starting.\n",
      "1th epoch starting.\n",
      "2th epoch starting.\n",
      "3th epoch starting.\n",
      "4th epoch starting.\n",
      "5th epoch starting.\n",
      "6th epoch starting.\n",
      "7th epoch starting.\n",
      "8th epoch starting.\n",
      "9th epoch starting.\n",
      "Time ellapsed in training is: 55.11276817321777\n",
      "[Test set] Average loss: 0.0004, Accuracy: 9859/10000 (98.59%)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# use of C3_layer_full\n",
    "\n",
    "'''\n",
    "Step 3\n",
    "'''\n",
    "model = LeNet().to(device)\n",
    "loss_function = torch.nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=1e-1)\n",
    "\n",
    "# print total number of trainable parameters\n",
    "param_ct = sum([p.numel() for p in model.parameters()])\n",
    "print(f\"Total number of trainable parameters: {param_ct}\")\n",
    "\n",
    "'''\n",
    "Step 4\n",
    "'''\n",
    "train_loader = torch.utils.data.DataLoader(dataset=train_dataset, batch_size=100, shuffle=True)\n",
    "\n",
    "import time\n",
    "start = time.time()\n",
    "for epoch in range(10) :\n",
    "    print(\"{}th epoch starting.\".format(epoch))\n",
    "    for images, labels in train_loader :\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        train_loss = loss_function(model(images), labels)\n",
    "        train_loss.backward()\n",
    "\n",
    "        optimizer.step()\n",
    "end = time.time()\n",
    "print(\"Time ellapsed in training is: {}\".format(end - start))\n",
    "\n",
    "'''\n",
    "Step 5\n",
    "'''\n",
    "test_loss, correct, total = 0, 0, 0\n",
    "\n",
    "test_loader = torch.utils.data.DataLoader(dataset=test_dataset, batch_size=100, shuffle=False)\n",
    "\n",
    "for images, labels in test_loader :\n",
    "    images, labels = images.to(device), labels.to(device)\n",
    "\n",
    "    output = model(images)\n",
    "    test_loss += loss_function(output, labels).item()\n",
    "\n",
    "    pred = output.max(1, keepdim=True)[1]\n",
    "    correct += pred.eq(labels.view_as(pred)).sum().item()\n",
    "    \n",
    "    total += labels.size(0)\n",
    "            \n",
    "print('[Test set] Average loss: {:.4f}, Accuracy: {}/{} ({:.2f}%)\\n'.format(\n",
    "        test_loss /total, correct, total,\n",
    "        100. * correct / total))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of trainable parameters: 60806\n",
      "0th epoch starting.\n",
      "1th epoch starting.\n",
      "2th epoch starting.\n",
      "3th epoch starting.\n",
      "4th epoch starting.\n",
      "5th epoch starting.\n",
      "6th epoch starting.\n",
      "7th epoch starting.\n",
      "8th epoch starting.\n",
      "9th epoch starting.\n",
      "Time ellapsed in training is: 131.87913489341736\n",
      "[Test set] Average loss: 0.0004, Accuracy: 9873/10000 (98.73%)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Case of using C3_layer (original)\n",
    "\n",
    "'''\n",
    "Step 3\n",
    "'''\n",
    "model = LeNet_original().to(device)\n",
    "loss_function = torch.nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=1e-1)\n",
    "\n",
    "# print total number of trainable parameters\n",
    "param_ct = sum([p.numel() for p in model.parameters()])\n",
    "print(f\"Total number of trainable parameters: {param_ct}\")\n",
    "\n",
    "'''\n",
    "Step 4\n",
    "'''\n",
    "train_loader = torch.utils.data.DataLoader(dataset=train_dataset, batch_size=100, shuffle=True)\n",
    "\n",
    "import time\n",
    "start = time.time()\n",
    "for epoch in range(10) :\n",
    "    print(\"{}th epoch starting.\".format(epoch))\n",
    "    for images, labels in train_loader :\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        train_loss = loss_function(model(images), labels)\n",
    "        train_loss.backward()\n",
    "\n",
    "        optimizer.step()\n",
    "end = time.time()\n",
    "print(\"Time ellapsed in training is: {}\".format(end - start))\n",
    "\n",
    "'''\n",
    "Step 5\n",
    "'''\n",
    "test_loss, correct, total = 0, 0, 0\n",
    "\n",
    "test_loader = torch.utils.data.DataLoader(dataset=test_dataset, batch_size=100, shuffle=False)\n",
    "\n",
    "for images, labels in test_loader :\n",
    "    images, labels = images.to(device), labels.to(device)\n",
    "\n",
    "    output = model(images)\n",
    "    test_loss += loss_function(output, labels).item()\n",
    "\n",
    "    pred = output.max(1, keepdim=True)[1]\n",
    "    correct += pred.eq(labels.view_as(pred)).sum().item()\n",
    "    \n",
    "    total += labels.size(0)\n",
    "            \n",
    "print('[Test set] Average loss: {:.4f}, Accuracy: {}/{} ({:.2f}%)\\n'.format(\n",
    "        test_loss /total, correct, total,\n",
    "        100. * correct / total))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "parameter reduction:  900\n"
     ]
    }
   ],
   "source": [
    "print(\"parameter reduction: \", 61706 - 60806)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of parameter of full C3:  2416\n",
      "Number of parameter of original C3:  1516\n",
      "Number of parameter reduction agree expect from hand calculations.\n"
     ]
    }
   ],
   "source": [
    "print(\"Number of parameter of full C3: \", 6 * 16 * 5 * 5 + 16)\n",
    "print(\"Number of parameter of original C3: \", (3 * 1 * 5 * 5 + 1) * 6 + (4 * 1 * 5 * 5 + 1) * 9 + (6 * 1 * 5 * 5 + 1) * 1)\n",
    "print(\"Number of parameter reduction agree expect from hand calculations.\")"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "1918e439edc4fef4c1c3eecf8a7ece8936a67f1982afb40c918b57a6d73fc50f"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6rc1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
